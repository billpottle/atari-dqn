{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atari DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/billpottle/atari-dqn/blob/master/Atari_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z2s3Tgxvq8u",
        "colab_type": "text"
      },
      "source": [
        "A basic tensorflow 2.0 implementation of some of the ideas in DeepMind's papers, \"[Human Level Control Through Deep Reinforcement Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)\" and [Playing Atari with Deep Reinforcement Learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learningusing) Deep Q Learning to play Atari games. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGE8DO-xvsmM",
        "colab_type": "text"
      },
      "source": [
        "**References** - While the papers themselves are of course the most important references, I owe a lot to various reference implementations, notably Phil Tabor's [youtube video](https://www.youtube.com/watch?v=a5XbO5Qgy5w) and [Udemy Course](https://www.udemy.com/course/deep-q-learning-from-paper-to-code/). Although it is in tensorflow 1, I highly recommend this [extremely thorough explanation](https://colab.research.google.com/github/fg91/Deep-Q-Learning/blob/master/DQN.ipynb) with code that trains very quickly. I also recommend this [great medium article ](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC6tWuvNvwqz",
        "colab_type": "text"
      },
      "source": [
        "You can follow along and run the code by clicking on the 'play' icon to the left of each cell, or click 'Run all' from the runtime menu. \n",
        "\n",
        "The first thing we will do is import the various libraries that we will need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neG4f1IHUutt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e50e96e2-ff3f-42fc-8b1e-5d60cbc6f093"
      },
      "source": [
        "import keras.backend.tensorflow_backend as tfback\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import imageio\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "from skimage.transform import resize\n",
        "from keras.layers import Dense, Activation, Conv2D, Flatten\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ye5-HzxwA-M",
        "colab_type": "text"
      },
      "source": [
        "Next we will set our global variables. You can play around with different values and try to get a new high score. The network should be able to [train any available Atari game](https://gym.openai.com/envs/#atari). We usually want the deterministic-v4 environments. Make sure whichever game you want is the only one not commented out. Although this should be able to train any game, the model is game specific. A model trained on one game can't play other games. \n",
        "\n",
        "Variables in all caps are constants that don't change during the run. You can change them and see how it changes the results and training time. Don't change anything in lower case, as the program will change them during the run. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZTchNmLc_Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Where to save the model and gif files\n",
        "PATH = \"output/\"  \n",
        "os.makedirs(PATH, exist_ok=True)\n",
        "\n",
        "frames_for_gif = []\n",
        "evalulation = False #Don't change here, it will\n",
        "\n",
        "ENV_NAME = 'PongNoFrameskip-v4'\n",
        "#ENV_NAME = 'KungFuMasterNoFrameskip-v4'\n",
        "#ENV_NAME = 'BreakoutDeterministic-v4'\n",
        "#ENV_NAME = 'MsPacmanNoFrameskip-v4'\n",
        "\n",
        "# Take the following actions once per the specified number of games.\n",
        "EVALUATION_FREQUENCY = 60\n",
        "SAVE_FREQUENCY = 300\n",
        "MODEL_COPY_FREQUENCY = 3\n",
        "\n",
        "# Size of the memory in frames. Reduce this if you get out of RAM errors\n",
        "MAX_MEMORY_LENGTH = 15000\n",
        "BATCH_SIZE = 32 #How many memory samples to train on\n",
        "    \n",
        "NUM_GAMES = 2000 #Reduce this if it takes too long to train, increase it if\n",
        "# you need more time to learn. \n",
        "\n",
        "FRAME_HEIGHT = 80 #Smaller frames reduce training time but could lose info\n",
        "FRAME_WIDTH = 80 #Especially for smaller items like knives or dots \n",
        "\n",
        "INITIAL_EXP = 15000 #Number of random actions to start out with\n",
        "eps = 1.0 # Chance to take a random action\n",
        "EPS_MIN = 0.05 #Mimimum chance to take a random action\n",
        "EPS_DEC = 0.00001 #Amount to decrease random chance every frame\n",
        "\n",
        "n_frames = 0 # The cumulative number of frames the agent has seen\n",
        "\n",
        "ALPHA = 0.0001 # Learning rate -  how fast network parameters are updated.\n",
        "# increasing this will let you learn faster, but you may 'overshoot'\n",
        "GAMMA = 0.99 # Future reward discount\n",
        "\n",
        "# In case you want to save a model file and then train it more later. \n",
        "LOAD_CHECKPOINT = False\n",
        "LOAD_FILE = 'atari_model.h5'\n",
        "\n",
        "# We define this function so recent versions of keras and tf will work together\n",
        "def _get_available_gpus():\n",
        "  #global _LOCAL_DEVICES\n",
        "  if tfback._LOCAL_DEVICES is None:\n",
        "    devices = tf.config.list_logical_devices()\n",
        "    tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "  return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "tfback._get_available_gpus = _get_available_gpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i00XUUe1xjJG",
        "colab_type": "text"
      },
      "source": [
        "This type of training would take a LONG time on even the fastest CPU. GPUs developed for video games are good at doing many simple things at once, and thus they are indispensible for training neural networks. Under Runtime, go to 'Change runtime type' and make sure you are using a GPU. The following code will tell you which GPU you have. \n",
        "\n",
        "Google will assign you one and let you use it for up to 12 hours. For $10/mo, [Colab Pro](https://colab.research.google.com/signup), will give you more powerful GPUs, more memory, and 24 hour runtimes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KCSzqXEmKx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "35ea513e-4c1b-48c8-bcf8-1bf43840ad12"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 26 13:54:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMKKy9pyx5Ol",
        "colab_type": "text"
      },
      "source": [
        "**Random Agent** - The following code has a Random Agent play the game and then displays the last frame of the game. This can be useful if you are thinking about cropping the image to remove things like the score, level, or health bars.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKL_asTjdPcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "2347aefb-d7fd-4f76-f884-8efce50cbb8e"
      },
      "source": [
        "# Create the environment\n",
        "env = gym.make(ENV_NAME).env\n",
        "# Reset it, returns the starting frame\n",
        "frame = env.reset()\n",
        "print(\"To play this game, on each frame the player chooses one of {} possible actions:\".format(env.action_space.n))\n",
        "print(\"{}\".format(env.unwrapped.get_action_meanings()))\n",
        "done = False\n",
        "while not done:\n",
        "  # Perform a random action, returns the new frame, reward and whether the game is over\n",
        "  frame, reward, done, info = env.step(env.action_space.sample())\n",
        "env.close() \n",
        "plt.imshow(frame)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To play this game, on each frame the player chooses one of 6 possible actions:\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe1aaa54080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0jhVb7Syr11",
        "colab_type": "text"
      },
      "source": [
        "Going from 3 channels (RGB) to 1 (BW) gives us a 3x reduction in size. Cropping and scaling from 210 X 160 to 80 X 80 gives us a further 5.25 reduction in size. So we can train something in 1 day that would take 15.75 days on a full frame. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpu_B0fJdT7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "b485279f-8463-4c23-b2a7-7bf1db54a62f"
      },
      "source": [
        "print(\"Original Frame Size:\", frame.shape)\n",
        "processed = tf.image.rgb_to_grayscale(frame)\n",
        "processed = tf.image.resize(processed,[FRAME_HEIGHT, FRAME_WIDTH],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "processed = np.asarray(processed).reshape(FRAME_HEIGHT,FRAME_WIDTH)\n",
        "processed = np.asarray(processed)\n",
        "plt.gray()\n",
        "plt.imshow(processed) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Frame Size: (210, 160, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe1a00d0f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVGbFvS0W1l",
        "colab_type": "text"
      },
      "source": [
        "**Exploration vs Exploitation** - Say there are 100 restaurants in your town. You've been to the 10 nearby, and really like 2 or 3. When you finally get that date night, do you hit up one that you know will be good, or do you venture out to try something new? If you never try something new, there is a 90% chance that you will miss the best restaurant out there. \n",
        "\n",
        "Our agent has a similar issue. Should it try the move it thinks is best, or try a random move with the chance to come up with a better strategy? \n",
        "\n",
        "The following code will tell it which to do - When we start out, we are going to do only random actions. After some amount of time, we are going to slowly decrease the amount of random actions we take. We will reach a plateau, though, because we always want to be taking at least a few random chances. \n",
        "\n",
        "Of course, during evaulation mode, we want the agent to only do what it thinks is best, so we will never take random actions when we are testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDr48tOGVnfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ExplorerExploiter():\n",
        "\n",
        "  # Never take a random action during evaluation games\n",
        "  if evaluation: \n",
        "    return 'Exploit'\n",
        "\n",
        "  # Take only random actions during initial exploration phase\n",
        "  if n_frames < INITIAL_EXP:\n",
        "    return 'Explore'\n",
        "\n",
        "  # Explore with chance eps\n",
        "  if np.random.random() < eps:\n",
        "    return 'Explore'\n",
        "\n",
        "  # Otherwise exploit\n",
        "  return 'Exploit'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e5BOLa0fKe",
        "colab_type": "text"
      },
      "source": [
        "The main idea behind deep Q learning is to create a neural network that takes the pixels of the game board in as input and outputs the best action. The final output layer has one node for each possible action - the value of the node for the input pixels is what value we expect to get from taking that action (ie, fire, jump, turn left) in the given state (input of all pixels on the screen). **When training, we continually adjust the network so that the weights are close to the actual value received for taking a move in a given state.** Then, for new states, we pick the move that network says will give us the highest reward + future reward. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLWchMAE0jGG",
        "colab_type": "text"
      },
      "source": [
        "We could train on each frame or game, but it's much more effecient to create a Memory that has a large number of previous frames and their results. When training, we simply sample from that memory.\n",
        "\n",
        "Each 'memory' consists of the state (pixels), what action we took, what reward we got, what the next state was, and whether or not the game ended. If you are running the notebook and crash due to a memory error, you should still be able to get good results with a smaller memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-3-c8wOMYM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExperienceMemory(object):\n",
        "  def __init__(self, max_memory_length):\n",
        "      self.state_memory = deque(maxlen=max_memory_length)\n",
        "      self.action_memory = deque(maxlen=max_memory_length)\n",
        "      self.reward_memory = deque(maxlen=max_memory_length)\n",
        "      self.next_state_memory = deque(maxlen=max_memory_length)\n",
        "      self.done_memory = deque(maxlen=max_memory_length)\n",
        "      \n",
        "  def get_length(self):\n",
        "      return len(self.state_memory)\n",
        "\n",
        "  # Store a new memory    \n",
        "  def store_transition(self, state, action, reward, next_state, done):\n",
        "      self.state_memory.append(state)\n",
        "      self.action_memory.append(action)\n",
        "      self.reward_memory.append(reward)\n",
        "      self.next_state_memory.append(next_state)\n",
        "      self.done_memory.append(done)\n",
        "\n",
        "  # Get out batch_size samples from the memory\n",
        "  def sample_buffer(self, batch_size):\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_states = []\n",
        "    dones = []\n",
        "  \n",
        "    for i in range(batch_size):\n",
        "      sample_id  = np.random.randint(len(self.state_memory))\n",
        "      states.append(self.state_memory[sample_id])\n",
        "      actions.append(self.action_memory[sample_id])\n",
        "      rewards.append(self.reward_memory[sample_id])\n",
        "      next_states.append(self.next_state_memory[sample_id])\n",
        "      dones.append(self.done_memory[sample_id])\n",
        "\n",
        "    return np.asarray(states), np.asarray(actions), np.asarray(rewards), np.asarray(next_states), np.asarray(dones)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAW2htlo1K5P",
        "colab_type": "text"
      },
      "source": [
        "**Atari Wrappers** - The Atari environment outputs 'raw' frames, so we need to apply these wrappers to format the frames to be what our agent should see. \n",
        "\n",
        "The Skip wrapper helps us pool the result from 4 frames, since sometimes the atari environments can skil\n",
        "\n",
        "The PreProcessFrame wrapper converts the frame to grayscale, while the move image channels wrapper gets the channels into a format our DQN can use. \n",
        "\n",
        "The Normalize Frame wrapper puts all pixel values to between 0 and 1, which helps with the math. We could also clip the reward - this helps on some games as well. \n",
        "\n",
        "The FrameStacker stacks the last 4 frames together - this is important because our agent needs to understand motion. With only one image, how could it tell which direction a ball was going? With 2 it could tell direction and speed, but not acceleration. Most implementations use 4 frames, because it is an even number and it gives some better historical information about the movement of objects. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsd1aLx_dye5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(SkipEnv, self).__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            t_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "         # When we are in evaluation mode, we need to send the deepest wrapped \n",
        "        # (original) observation back out so that we can see how the agent plays\n",
        "        if evaluation == True:\n",
        "          frames_for_gif.append(np.asarray(obs))\n",
        "        return obs, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer = []\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class PreProcessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(PreProcessFrame, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "                                                shape=(FRAME_HEIGHT,FRAME_WIDTH,1), dtype=np.uint8)\n",
        "    def observation(self, obs):\n",
        "        \n",
        "        new_frame = np.reshape(obs, obs.shape).astype(np.float32)\n",
        "        # convert to grayscale\n",
        "        new_frame = tf.image.rgb_to_grayscale(new_frame)\n",
        "        # scale to frame height and width\n",
        "        new_frame = tf.image.resize(new_frame,[FRAME_HEIGHT, FRAME_WIDTH],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        # convert to numpy array\n",
        "        new_frame = np.asarray(new_frame)\n",
        "        return new_frame.astype(np.float32)\n",
        "\n",
        "class MoveImgChannel(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(MoveImgChannel, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "                            shape=(self.observation_space.shape[-1],\n",
        "                                   self.observation_space.shape[0],\n",
        "                                   self.observation_space.shape[1]),\n",
        "                            dtype=np.float32)\n",
        "  \n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class NormalizeFrame(gym.ObservationWrapper):\n",
        "    # The match is easier if everything is normalized to be betwee 0 and 1.\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "class FrameStacker(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(FrameStacker, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                             env.observation_space.low.repeat(n_steps, axis=0),\n",
        "                             env.observation_space.high.repeat(n_steps, axis=0),\n",
        "                             dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "# Apply all the wrappers on top of each other.\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = SkipEnv(env)\n",
        "    env = PreProcessFrame(env)\n",
        "    env = MoveImgChannel(env)\n",
        "    env = FrameStacker(env, 4)\n",
        "    return NormalizeFrame(env)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27s7nZNL0Hs_",
        "colab_type": "text"
      },
      "source": [
        "Here is the code to generate and save the animated gifs. \n",
        "If using colab, you can click on files to the left and then output and download the gifs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB6LipnZh60C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_gif(frame_number, frames_for_gif, reward):\n",
        "\n",
        "    for idx, frame_idx in enumerate(frames_for_gif):\n",
        "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3),\n",
        "                                     preserve_range=True, order=0).astype(np.uint8)\n",
        "\n",
        "    imageio.mimsave(f'{PATH}{\"{0}_game_{1}_reward_{2}.gif\".format(ENV_NAME, frame_number, reward)}',\n",
        "                    frames_for_gif, duration=1/30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTFNcPQV2VOF",
        "colab_type": "text"
      },
      "source": [
        "**Agent** - Our Agent class is the key player. The agent has 2 neural networks, the Q network used for predicting the best action to take in each state, and the target network, used for predicting the value of that action. When there is only one network training is difficult because it's chasing after itself. The q network's values are periodically copied over to the target network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfl90rpgdtUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, alpha, n_actions, input_dims):\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.gamma = GAMMA\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.memory= ExperienceMemory(MAX_MEMORY_LENGTH)\n",
        "        self.q_network = self.build_dqn(alpha, n_actions, input_dims)\n",
        "        self.target_network = self.build_dqn(alpha, n_actions, input_dims)\n",
        "\n",
        "    # This is the same architecture used by deep mind\n",
        "    def build_dqn(self, lr, n_actions, input_dims):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(filters=32, kernel_size=8, strides=4, activation='relu',\n",
        "                        input_shape=(*input_dims,), data_format='channels_first'))\n",
        "        model.add(Conv2D(filters=64, kernel_size=4, strides=2, activation='relu',\n",
        "                        data_format='channels_first'))\n",
        "        model.add(Conv2D(filters=64, kernel_size=3, strides=1, activation='relu',\n",
        "                        data_format='channels_first'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(n_actions))\n",
        "\n",
        "        model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
        "        model.summary() # Print network details\n",
        "        return model\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    # The agent will make use of our ExplorerExploiter to choose either\n",
        "    # Random actions or the action that gives the highest Q value\n",
        "    def choose_action(self, observation):\n",
        "        \n",
        "        if ExplorerExploiter() == 'Explore': \n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            state = np.array([observation], copy=False, dtype=np.float32)\n",
        "            actions = self.q_network.predict(state)\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def learn(self):\n",
        "      # First of all, make sure we have enough memories to train on.\n",
        "        if self.memory.get_length() > self.batch_size:\n",
        "            # Get a batch of memories. Each is an array of 32 memories\n",
        "            state, action, reward, new_state, done = \\\n",
        "                                    self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "            #Predict both the values we thought we could get. Use the q\n",
        "            # network for the state and the target network for the next state\n",
        "\n",
        "            q_eval = self.q_network.predict(state)\n",
        "            q_next = self.target_network.predict(new_state)\n",
        "\n",
        "            q_target = q_eval[:]\n",
        "\n",
        "            indices = np.arange(self.batch_size)\n",
        "            # Dones is 0 or 1, so this acts as a mask so that when the episode\n",
        "            # is done, we will only take the reward\n",
        "            # When it is not done, we will take the best value reward of the\n",
        "            # next state times the future discount\n",
        "\n",
        "            q_target[indices, action] = reward + \\\n",
        "                                    self.gamma*np.max(q_next, axis=1)*(1 - done)\n",
        "            # finally, train the network to backpropogate the loss\n",
        "            self.q_network.train_on_batch(state, q_target)\n",
        "\n",
        "    def save_models(self):\n",
        "        self.q_network.save('Atari Model {}.h5'.format(ENV_NAME))\n",
        "        print('... saving models ...')\n",
        "    \n",
        "    # Restore the model and copy parameters to target network\n",
        "    def load_models(self):\n",
        "        self.q_network = load_model(SAVE_FILE)\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "        print('... loading models ...')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShzsHLlN5KFo",
        "colab_type": "text"
      },
      "source": [
        "**Main Loop** - There are two nested loops here - one for each frame of each game, and then the outer loop runs for the specified number of games. \n",
        "\n",
        "Finally, plot out the scores over time. Most agents follow a roughly logrithmic pattern, where they don't learn any more after a certain time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_otnehxdcPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "502f9a97-c543-4c74-d8c1-e7dd164dbc7c"
      },
      "source": [
        "# Make our environment\n",
        "env = make_env(ENV_NAME)\n",
        "\n",
        "# Make our agent\n",
        "agent = Agent(alpha=ALPHA, input_dims=(4,FRAME_HEIGHT,FRAME_WIDTH), n_actions=env.action_space.n)   \n",
        "\n",
        "# Set the worst possible best score\n",
        "best_score = - math.inf \n",
        "\n",
        "if LOAD_CHECKPOINT:\n",
        "    agent.load_models()\n",
        "\n",
        "scores  = []\n",
        " \n",
        "# Main training loop \n",
        "for i in range(NUM_GAMES):\n",
        "\n",
        "    # Check if it's time to play an evaluation game, save models, \n",
        "    # or copy network parameters   \n",
        "    if i != 0 and i % EVALUATION_FREQUENCY == 0:\n",
        "      evaluation = True\n",
        "    else: \n",
        "      evaluation = False\n",
        "\n",
        "    if i % MODEL_COPY_FREQUENCY == 0:\n",
        "      agent.target_network.set_weights(agent.q_network.get_weights())\n",
        "       \n",
        "    # Reset parameters for each game   \n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    score = 0\n",
        "\n",
        "    # Loop that runs for each frame of a game\n",
        "    while not done:\n",
        "        # Pick which action to take\n",
        "        action = agent.choose_action(observation)\n",
        "\n",
        "        # Receive the results - next observation (frame), reward, done, and \n",
        "        # info (not used here)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        n_frames += 1\n",
        "        score += reward\n",
        "\n",
        "        # See if it's time to update epsilon\n",
        "        if eps > EPS_MIN and n_frames > INITIAL_EXP:\n",
        "          eps = eps - EPS_DEC\n",
        "\n",
        "        # Store the memory\n",
        "        agent.store_transition(observation, action,\n",
        "                              reward, observation_, int(done))\n",
        "        # Train on one batch\n",
        "        agent.learn() \n",
        "        # Increment observation\n",
        "        observation = observation_\n",
        "\n",
        "\n",
        "    # When game is over, add to scores\n",
        "    scores.append(score)\n",
        "\n",
        "    # If it was an evaluation game, record gif\n",
        "    if i!= 0 and i % EVALUATION_FREQUENCY == 0:\n",
        "      generate_gif(i, frames_for_gif, score)\n",
        "      frames_for_gif = []\n",
        "    \n",
        "\n",
        "    # Update average scores\n",
        "    avg_score = np.mean(scores[-50:])\n",
        "        \n",
        "    print('game: ', i,'score: ', score,\n",
        "         ' average score %.3f' % np.mean(scores[-50:]),\n",
        "        'epsilon %.2f' % eps, 'steps', n_frames)\n",
        "    \n",
        "    # Print a message. Many of these over time is an indication of learning.\n",
        "    if avg_score > best_score:\n",
        "        print('Go you! - last 50 games avg score %.2f better than best 50 games avg %.2f. ' % (\n",
        "              avg_score, best_score))\n",
        "        best_score = avg_score\n",
        "\n",
        "\n",
        "# Plot the final results    \n",
        "env.close()\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Game Number')\n",
        "plt.plot(scores)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 19, 19)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 8, 8)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 6, 6)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 1,261,222\n",
            "Trainable params: 1,261,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 32, 19, 19)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 64, 8, 8)          32832     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 64, 6, 6)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 3078      \n",
            "=================================================================\n",
            "Total params: 1,261,222\n",
            "Trainable params: 1,261,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "game:  0 score:  -21.0  average score -21.000 epsilon 1.00 steps 826\n",
            "Go you! - last 50 games avg score -21.00 better than best 50 games avg -inf. \n",
            "game:  1 score:  -20.0  average score -20.500 epsilon 1.00 steps 1825\n",
            "Go you! - last 50 games avg score -20.50 better than best 50 games avg -21.00. \n",
            "game:  2 score:  -19.0  average score -20.000 epsilon 1.00 steps 2894\n",
            "Go you! - last 50 games avg score -20.00 better than best 50 games avg -20.50. \n",
            "game:  3 score:  -21.0  average score -20.250 epsilon 1.00 steps 3774\n",
            "game:  4 score:  -20.0  average score -20.200 epsilon 1.00 steps 4751\n",
            "game:  5 score:  -21.0  average score -20.333 epsilon 1.00 steps 5575\n",
            "game:  6 score:  -21.0  average score -20.429 epsilon 1.00 steps 6367\n",
            "game:  7 score:  -21.0  average score -20.500 epsilon 1.00 steps 7159\n",
            "game:  8 score:  -21.0  average score -20.556 epsilon 1.00 steps 7979\n",
            "game:  9 score:  -20.0  average score -20.500 epsilon 1.00 steps 9035\n",
            "game:  10 score:  -18.0  average score -20.273 epsilon 1.00 steps 10081\n",
            "game:  11 score:  -21.0  average score -20.333 epsilon 1.00 steps 10907\n",
            "game:  12 score:  -21.0  average score -20.385 epsilon 1.00 steps 11835\n",
            "game:  13 score:  -21.0  average score -20.429 epsilon 1.00 steps 12659\n",
            "game:  14 score:  -21.0  average score -20.467 epsilon 1.00 steps 13481\n",
            "game:  15 score:  -21.0  average score -20.500 epsilon 1.00 steps 14292\n",
            "game:  16 score:  -19.0  average score -20.412 epsilon 1.00 steps 15268\n",
            "game:  17 score:  -20.0  average score -20.389 epsilon 0.99 steps 16157\n",
            "game:  18 score:  -21.0  average score -20.421 epsilon 0.98 steps 16970\n",
            "game:  19 score:  -21.0  average score -20.450 epsilon 0.97 steps 17938\n",
            "game:  20 score:  -21.0  average score -20.476 epsilon 0.96 steps 18730\n",
            "game:  21 score:  -21.0  average score -20.500 epsilon 0.96 steps 19494\n",
            "game:  22 score:  -21.0  average score -20.522 epsilon 0.95 steps 20337\n",
            "game:  23 score:  -20.0  average score -20.500 epsilon 0.94 steps 21269\n",
            "game:  24 score:  -21.0  average score -20.520 epsilon 0.93 steps 22121\n",
            "game:  25 score:  -21.0  average score -20.538 epsilon 0.92 steps 23007\n",
            "game:  26 score:  -21.0  average score -20.556 epsilon 0.91 steps 23850\n",
            "game:  27 score:  -21.0  average score -20.571 epsilon 0.90 steps 24676\n",
            "game:  28 score:  -21.0  average score -20.586 epsilon 0.89 steps 25712\n",
            "game:  29 score:  -21.0  average score -20.600 epsilon 0.88 steps 26594\n",
            "game:  30 score:  -20.0  average score -20.581 epsilon 0.88 steps 27437\n",
            "game:  31 score:  -20.0  average score -20.562 epsilon 0.87 steps 28367\n",
            "game:  32 score:  -18.0  average score -20.485 epsilon 0.85 steps 29543\n",
            "game:  33 score:  -21.0  average score -20.500 epsilon 0.84 steps 30545\n",
            "game:  34 score:  -21.0  average score -20.514 epsilon 0.84 steps 31397\n",
            "game:  35 score:  -19.0  average score -20.472 epsilon 0.83 steps 32500\n",
            "game:  36 score:  -19.0  average score -20.432 epsilon 0.81 steps 33679\n",
            "game:  37 score:  -21.0  average score -20.447 epsilon 0.81 steps 34462\n",
            "game:  38 score:  -19.0  average score -20.410 epsilon 0.80 steps 35470\n",
            "game:  39 score:  -21.0  average score -20.425 epsilon 0.79 steps 36354\n",
            "game:  40 score:  -21.0  average score -20.439 epsilon 0.78 steps 37234\n",
            "game:  41 score:  -20.0  average score -20.429 epsilon 0.77 steps 38176\n",
            "game:  42 score:  -21.0  average score -20.442 epsilon 0.76 steps 38987\n",
            "game:  43 score:  -20.0  average score -20.432 epsilon 0.75 steps 39996\n",
            "game:  44 score:  -20.0  average score -20.422 epsilon 0.74 steps 41038\n",
            "game:  45 score:  -21.0  average score -20.435 epsilon 0.73 steps 41888\n",
            "game:  46 score:  -21.0  average score -20.447 epsilon 0.72 steps 42802\n",
            "game:  47 score:  -20.0  average score -20.438 epsilon 0.71 steps 43733\n",
            "game:  48 score:  -21.0  average score -20.449 epsilon 0.70 steps 44614\n",
            "game:  49 score:  -18.0  average score -20.400 epsilon 0.69 steps 45826\n",
            "game:  50 score:  -21.0  average score -20.400 epsilon 0.68 steps 46794\n",
            "game:  51 score:  -19.0  average score -20.380 epsilon 0.67 steps 48093\n",
            "game:  52 score:  -17.0  average score -20.340 epsilon 0.66 steps 49287\n",
            "game:  53 score:  -17.0  average score -20.260 epsilon 0.64 steps 50616\n",
            "game:  54 score:  -20.0  average score -20.260 epsilon 0.63 steps 51626\n",
            "game:  55 score:  -19.0  average score -20.220 epsilon 0.62 steps 52912\n",
            "game:  56 score:  -20.0  average score -20.200 epsilon 0.61 steps 53878\n",
            "game:  57 score:  -19.0  average score -20.160 epsilon 0.60 steps 55090\n",
            "game:  58 score:  -19.0  average score -20.120 epsilon 0.59 steps 56268\n",
            "game:  59 score:  -20.0  average score -20.120 epsilon 0.58 steps 57181\n",
            "game:  60 score:  -19.0  average score -20.140 epsilon 0.56 steps 58653\n",
            "game:  61 score:  -19.0  average score -20.100 epsilon 0.55 steps 59804\n",
            "game:  62 score:  -19.0  average score -20.060 epsilon 0.54 steps 60906\n",
            "game:  63 score:  -21.0  average score -20.060 epsilon 0.53 steps 62251\n",
            "game:  64 score:  -19.0  average score -20.020 epsilon 0.52 steps 63435\n",
            "game:  65 score:  -16.0  average score -19.920 epsilon 0.50 steps 64913\n",
            "Go you! - last 50 games avg score -19.92 better than best 50 games avg -20.00. \n",
            "game:  66 score:  -18.0  average score -19.900 epsilon 0.49 steps 66235\n",
            "Go you! - last 50 games avg score -19.90 better than best 50 games avg -19.92. \n",
            "game:  67 score:  -18.0  average score -19.860 epsilon 0.47 steps 67606\n",
            "Go you! - last 50 games avg score -19.86 better than best 50 games avg -19.90. \n",
            "game:  68 score:  -18.0  average score -19.800 epsilon 0.46 steps 68892\n",
            "Go you! - last 50 games avg score -19.80 better than best 50 games avg -19.86. \n",
            "game:  69 score:  -17.0  average score -19.720 epsilon 0.45 steps 70265\n",
            "Go you! - last 50 games avg score -19.72 better than best 50 games avg -19.80. \n",
            "game:  70 score:  -15.0  average score -19.600 epsilon 0.43 steps 71955\n",
            "Go you! - last 50 games avg score -19.60 better than best 50 games avg -19.72. \n",
            "game:  71 score:  -21.0  average score -19.600 epsilon 0.42 steps 73120\n",
            "game:  72 score:  -18.0  average score -19.540 epsilon 0.40 steps 74514\n",
            "Go you! - last 50 games avg score -19.54 better than best 50 games avg -19.60. \n",
            "game:  73 score:  -17.0  average score -19.480 epsilon 0.39 steps 76196\n",
            "Go you! - last 50 games avg score -19.48 better than best 50 games avg -19.54. \n",
            "game:  74 score:  -17.0  average score -19.400 epsilon 0.37 steps 77854\n",
            "Go you! - last 50 games avg score -19.40 better than best 50 games avg -19.48. \n",
            "game:  75 score:  -20.0  average score -19.380 epsilon 0.36 steps 79206\n",
            "Go you! - last 50 games avg score -19.38 better than best 50 games avg -19.40. \n",
            "game:  76 score:  -20.0  average score -19.360 epsilon 0.35 steps 80454\n",
            "Go you! - last 50 games avg score -19.36 better than best 50 games avg -19.38. \n",
            "game:  77 score:  -17.0  average score -19.280 epsilon 0.33 steps 82062\n",
            "Go you! - last 50 games avg score -19.28 better than best 50 games avg -19.36. \n",
            "game:  78 score:  -18.0  average score -19.220 epsilon 0.31 steps 83596\n",
            "Go you! - last 50 games avg score -19.22 better than best 50 games avg -19.28. \n",
            "game:  79 score:  -15.0  average score -19.100 epsilon 0.30 steps 85398\n",
            "Go you! - last 50 games avg score -19.10 better than best 50 games avg -19.22. \n",
            "game:  80 score:  -17.0  average score -19.040 epsilon 0.28 steps 86908\n",
            "Go you! - last 50 games avg score -19.04 better than best 50 games avg -19.10. \n",
            "game:  81 score:  -20.0  average score -19.040 epsilon 0.27 steps 88362\n",
            "game:  82 score:  -15.0  average score -18.980 epsilon 0.25 steps 90303\n",
            "Go you! - last 50 games avg score -18.98 better than best 50 games avg -19.04. \n",
            "game:  83 score:  -15.0  average score -18.860 epsilon 0.23 steps 92102\n",
            "Go you! - last 50 games avg score -18.86 better than best 50 games avg -18.98. \n",
            "game:  84 score:  -15.0  average score -18.740 epsilon 0.21 steps 94164\n",
            "Go you! - last 50 games avg score -18.74 better than best 50 games avg -18.86. \n",
            "game:  85 score:  -14.0  average score -18.640 epsilon 0.19 steps 96121\n",
            "Go you! - last 50 games avg score -18.64 better than best 50 games avg -18.74. \n",
            "game:  86 score:  -20.0  average score -18.660 epsilon 0.17 steps 97768\n",
            "game:  87 score:  -18.0  average score -18.600 epsilon 0.15 steps 99516\n",
            "Go you! - last 50 games avg score -18.60 better than best 50 games avg -18.64. \n",
            "game:  88 score:  -20.0  average score -18.620 epsilon 0.14 steps 101088\n",
            "game:  89 score:  -16.0  average score -18.520 epsilon 0.12 steps 103134\n",
            "Go you! - last 50 games avg score -18.52 better than best 50 games avg -18.60. \n",
            "game:  90 score:  -17.0  average score -18.440 epsilon 0.10 steps 105155\n",
            "Go you! - last 50 games avg score -18.44 better than best 50 games avg -18.52. \n",
            "game:  91 score:  -14.0  average score -18.320 epsilon 0.08 steps 107495\n",
            "Go you! - last 50 games avg score -18.32 better than best 50 games avg -18.44. \n",
            "game:  92 score:  -17.0  average score -18.240 epsilon 0.05 steps 109728\n",
            "Go you! - last 50 games avg score -18.24 better than best 50 games avg -18.32. \n",
            "game:  93 score:  -18.0  average score -18.200 epsilon 0.05 steps 111582\n",
            "Go you! - last 50 games avg score -18.20 better than best 50 games avg -18.24. \n",
            "game:  94 score:  -18.0  average score -18.160 epsilon 0.05 steps 113351\n",
            "Go you! - last 50 games avg score -18.16 better than best 50 games avg -18.20. \n",
            "game:  95 score:  -16.0  average score -18.060 epsilon 0.05 steps 115308\n",
            "Go you! - last 50 games avg score -18.06 better than best 50 games avg -18.16. \n",
            "game:  96 score:  -14.0  average score -17.920 epsilon 0.05 steps 117434\n",
            "Go you! - last 50 games avg score -17.92 better than best 50 games avg -18.06. \n",
            "game:  97 score:  -9.0  average score -17.700 epsilon 0.05 steps 120195\n",
            "Go you! - last 50 games avg score -17.70 better than best 50 games avg -17.92. \n",
            "game:  98 score:  -18.0  average score -17.640 epsilon 0.05 steps 122479\n",
            "Go you! - last 50 games avg score -17.64 better than best 50 games avg -17.70. \n",
            "game:  99 score:  -14.0  average score -17.560 epsilon 0.05 steps 124402\n",
            "Go you! - last 50 games avg score -17.56 better than best 50 games avg -17.64. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cec3f789c045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Receive the results - next observation (frame), reward, done, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# info (not used here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mn_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bab403910114>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mt_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}